{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "client_key = 'izGsqP2exeThwwEUVU3x'\n",
    "client_secret = 'WrwbQ1l6ZI'\n",
    "query = '진료'\n",
    "encText = urllib.parse.quote_plus(query) # quote를 써도됨 공백이 없기 때문에  \n",
    "num = 100\n",
    "naver_url = 'https://openapi.naver.com/v1/search/blog.json?query=' + encText + '&display=' + str(num)\n",
    "\n",
    "text = []\n",
    "request = urllib.request.Request(naver_url)\n",
    "request.add_header(\"X-Naver-Client-Id\",client_key)\n",
    "request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "response = urllib.request.urlopen(request)\n",
    "rescode = response.getcode()\n",
    "if(rescode == 200):\n",
    "    response_body = response.read()\n",
    "    dataList = json.loads(response_body)\n",
    "    count = 1\n",
    "    print('[' + query + '에 대한 네이버 블로그 글 ]')\n",
    "        \n",
    "    for data in dataList['items'] :\n",
    "        print (str(count) + ' : ' + data['title'], end=\" \")\n",
    "        text.append(data['title'])\n",
    "        #print ('[' + data['description'] + ']')\n",
    "        text.append(data['description'])\n",
    "        count += 1\n",
    "else:\n",
    "    print('오류 코드 : ' + rescode)\n",
    "    \n",
    "wfile = open(\"output/collect.txt\",\"at\", encoding=\"utf-8\") \n",
    "wfile.writelines(text) \n",
    "wfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "client_key = 'izGsqP2exeThwwEUVU3x'\n",
    "client_secret = 'WrwbQ1l6ZI'\n",
    "query = '진료'\n",
    "encText = urllib.parse.quote_plus(query)\n",
    "\n",
    "num = 100\n",
    "naver_url = 'https://openapi.naver.com/v1/search/news.json?query=' + encText + '&display=' + str(num)\n",
    "\n",
    "text = []\n",
    "request = urllib.request.Request(naver_url)\n",
    "request.add_header(\"X-Naver-Client-Id\",client_key)\n",
    "request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "response = urllib.request.urlopen(request)\n",
    "\n",
    "rescode = response.getcode()\n",
    "\n",
    "if(rescode == 200):\n",
    "    response_body = response.read()\n",
    "    dataList = json.loads(response_body)\n",
    "    count = 1\n",
    "    print('[' + query + '에 대한 네이버 뉴스 글 ]')\n",
    "    for data in dataList['items'] :\n",
    "        print (str(count) + ' : ' + data['title'], end=\" \")\n",
    "        text.append(data['title'])\n",
    "        #print ('[' + data['description'] + ']')\n",
    "        text.append(data['description'])\n",
    "        count += 1\n",
    "else:\n",
    "    print('오류 코드 : ' + rescode)\n",
    "\n",
    "wfile = open(\"output/collect.txt\",\"at\", encoding=\"utf-8\") \n",
    "wfile.writelines(text) \n",
    "wfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "client_key = 'izGsqP2exeThwwEUVU3x'\n",
    "client_secret = 'WrwbQ1l6ZI'\n",
    "query = '병원'\n",
    "encText = urllib.parse.quote_plus(query) # quote를 써도됨 공백이 없기 때문에  \n",
    "num = 100\n",
    "naver_url = 'https://openapi.naver.com/v1/search/blog.json?query=' + encText + '&display=' + str(num)\n",
    "\n",
    "text = []\n",
    "request = urllib.request.Request(naver_url)\n",
    "request.add_header(\"X-Naver-Client-Id\",client_key)\n",
    "request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "response = urllib.request.urlopen(request)\n",
    "rescode = response.getcode()\n",
    "if(rescode == 200):\n",
    "    response_body = response.read()\n",
    "    dataList = json.loads(response_body)\n",
    "    count = 1\n",
    "    print('[' + query + '에 대한 네이버 블로그 글 ]')\n",
    "        \n",
    "    for data in dataList['items'] :\n",
    "        print (str(count) + ' : ' + data['title'], end=\" \")\n",
    "        text.append(data['title'])\n",
    "        #print ('[' + data['description'] + ']')\n",
    "        text.append(data['description'])\n",
    "        count += 1\n",
    "else:\n",
    "    print('오류 코드 : ' + rescode)\n",
    "    \n",
    "wfile = open(\"output/collect.txt\",\"at\", encoding=\"utf-8\") \n",
    "wfile.writelines(text) \n",
    "wfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "client_key = 'izGsqP2exeThwwEUVU3x'\n",
    "client_secret = 'WrwbQ1l6ZI'\n",
    "query = '병원'\n",
    "encText = urllib.parse.quote_plus(query)\n",
    "\n",
    "num = 100\n",
    "naver_url = 'https://openapi.naver.com/v1/search/news.json?query=' + encText + '&display=' + str(num)\n",
    "\n",
    "text = []\n",
    "request = urllib.request.Request(naver_url)\n",
    "request.add_header(\"X-Naver-Client-Id\",client_key)\n",
    "request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "response = urllib.request.urlopen(request)\n",
    "\n",
    "rescode = response.getcode()\n",
    "\n",
    "if(rescode == 200):\n",
    "    response_body = response.read()\n",
    "    dataList = json.loads(response_body)\n",
    "    count = 1\n",
    "    print('[' + query + '에 대한 네이버 뉴스 글 ]')\n",
    "    for data in dataList['items'] :\n",
    "        print (str(count) + ' : ' + data['title'], end=\" \")\n",
    "        text.append(data['title'])\n",
    "        #print ('[' + data['description'] + ']')\n",
    "        text.append(data['description'])\n",
    "        count += 1\n",
    "else:\n",
    "    print('오류 코드 : ' + rescode)\n",
    "\n",
    "wfile = open(\"output/collect.txt\",\"at\", encoding=\"utf-8\") \n",
    "wfile.writelines(text) \n",
    "wfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys \n",
    "import time\n",
    "import re\n",
    "\n",
    "url = \"https://search.naver.com/search.naver?where=kin&sm=tab_jum&query=%EC%A7%84%EB%A3%8C\"\n",
    "\n",
    "driver = webdriver.Chrome('C:/Temp/chromedriver')\n",
    "driver.implicitly_wait(5) \n",
    "\n",
    "driver.get(url) \n",
    "\n",
    "works = []\n",
    "while True:\n",
    "    titles = driver.find_elements_by_css_selector(\"#main_pack > section > div > ul > li > div > div.question_area > div > a\")\n",
    "\n",
    "    reviews = driver.find_elements_by_css_selector(\"#main_pack > section > div > ul > li > div > div.answer_area > div.answer_group > a\")\n",
    "\n",
    "    for title in titles:\n",
    "        #print(title.text,sep='')\n",
    "        works.append(title.text)\n",
    "    for review in reviews:\n",
    "        #print(review.text,sep='', end=' ')\n",
    "        works.append(title.text)\n",
    "        \n",
    "    nextbtn = driver.find_element_by_css_selector(\"#main_pack > div.api_sc_page_wrap > div > a.btn_next\")\n",
    "    if nextbtn.get_attribute('aria-disabled') == \"true\":\n",
    "        print(\"work finished\")\n",
    "        break\n",
    "    else:\n",
    "        driver.execute_script(\"arguments[0].click();\", nextbtn)\n",
    "        time.sleep(1)\n",
    "    \n",
    "wfile = open(\"output/collect.txt\",\"at\", encoding=\"utf-8\") \n",
    "wfile.writelines(works) \n",
    "wfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt    \n",
    "from konlpy.tag import Hannanum      \n",
    "from konlpy.tag import Kkma      \n",
    "from konlpy.utils import pprint\n",
    "from wordcloud import WordCloud  \n",
    "import nltk      \n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('output/collect.txt', 'r', encoding='utf-8')\n",
    "text = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(\"[^가-힣\\s]\", \"\", text)\n",
    "wfile = open(\"output/collect_edit.txt\",\"at\", encoding=\"utf-8\") \n",
    "wfile.writelines(text) \n",
    "wfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfontpath = \"data/THEdog.ttf\" \n",
    "okt = Okt()\n",
    "#hannanum = Hannanum()\n",
    "#kkma = Kkma()\n",
    "tokens_ko = okt.nouns(text)                ## 텍스트에서 명사를 추출한다. \n",
    "#tokens_ko = hannanum.nouns(text)\n",
    "#tokens_ko = kkma.nouns(text)\n",
    "print(tokens_ko[:30])\n",
    "\n",
    "nouns_text = nltk.Text(tokens_ko, name='naver hotel reviews')       ## 명사로 추출한 것을 텍스트 객체로 만든다 \n",
    "#nouns_text.vocab()\n",
    "nouns_text.vocab().most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokens_ko), 0):\n",
    "    if len(tokens_ko[i]) < 2:\n",
    "        del tokens_ko[i]\n",
    "\n",
    "nouns_text = nltk.Text(tokens_ko, name='naver hotel reviews')       ## 명사로 추출한 것을 텍스트 객체로 만든다 \n",
    "#nouns_text.vocab()\n",
    "nouns_text.vocab().most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = nouns_text.vocab().most_common(150) # 사용할 경우 dict로 형변환해서 넣어줘야함\n",
    "wc = WordCloud(font_path=myfontpath,\n",
    "                      relative_scaling = 0.2,\n",
    "                      width = 700,\n",
    "                      height = 500,\n",
    "                      colormap = 'coolwarm' # Set1, \n",
    "                      ).generate_from_frequencies(nouns_text.vocab())\n",
    "#wc.to_file('output/hw8.png')\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfile = open(\"output/count_words.txt\",\"w\", encoding=\"utf-8\") \n",
    "wfile.writelines(nouns_text.vocab()) \n",
    "wfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "title = []\n",
    "\n",
    "for i in range(1, 107):\n",
    "    req = requests.get(\"https://sldict.korean.go.kr/front/sign/signList.do?current_pos_index=&origin_no=0&searchWay=&top_category=&category=SPE003&detailCategory=&searchKeyword=&pageIndex=\"+str(i)+\"&pageJumpIndex=\")\n",
    "    html = req.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    titles = soup.select('#list > li:nth-of-type(1) > div > p > span.tit > a')\n",
    "\n",
    "    for dom in titles:\n",
    "        content=re.sub(\"[^가-힣\\s]\", '', dom.text) \n",
    "        title.append(content.strip())\n",
    "        #print(content.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = pd.DataFrame(title)\n",
    "titles.to_csv('output/list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "title = []\n",
    "\n",
    "#for i in range(1, 107):\n",
    "req = requests.get(\"http://signlanguage-dic.org/\")\n",
    "html = req.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "titles = soup.select('#content > div > div.content-list > section > div > article > a > header > h3')\n",
    "\n",
    "for dom in titles:\n",
    "    #content=re.sub(\"[^가-힣\\s]\", '', dom.text) \n",
    "    title.append(dom.text)\n",
    "    print(dom.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = pd.DataFrame(title)\n",
    "titles.to_csv('output/lists.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dic = pd.read_csv('output/list.csv')\n",
    "dic.columns=['abc']\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic2 = pd.read_csv('output/lists.csv', header=None)\n",
    "dic2.columns=['abc']\n",
    "dic2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(dic,dic2, on='abc', how='inner')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('output/result.csv', index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "from konlpy.utils import pprint\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pprint(okt.sentences(sen))\n",
    "\n",
    "pprint(okt.nouns(sen))\n",
    "\n",
    "pprint(okt.pos(sen))\n",
    "\n",
    "pprint(okt.morphs(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.read_csv('output/한국 수어 사전.csv', header = None)\n",
    "words.iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "keti = pd.read_csv('./data/keti2018(학습용).csv')\n",
    "keti_list = pd.read_csv('./data/keti_words.txt', sep='\\t', header=None)\n",
    "keti_lists = pd.DataFrame(keti_list.loc[:,4])\n",
    "keti1 = pd.DataFrame(keti.iloc[:,6])\n",
    "keti_lists.columns= ['한국어']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(keti,keti_lists, on='한국어', how='inner')\n",
    "result['언어 제공자 ID'].apply(lambda x : re.sub(\"[.]\", \"31\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('output/result_words.csv', index=False, header=None)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "keti = pd.read_csv('./data/keti2018(학습용).csv')\n",
    "keti_list = pd.read_csv('./data/keti_words_f.txt', sep='\\t', header=None)\n",
    "#keti_lists = pd.DataFrame(keti_list.loc[:,4])\n",
    "keti1 = pd.DataFrame(keti.iloc[:,6])\n",
    "keti_list.columns= ['한국어']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(keti,keti_list, on='한국어', how='inner')\n",
    "#result['언어 제공자 ID'].apply(lambda x : re.sub(\"[.]\", \"31\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.sort_values(by=['언어 제공자 ID', '번호']).to_csv('data/result_words.csv', index=False, header=None)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "keti = pd.read_csv('./data/keti2017.csv')\n",
    "keti_list = pd.read_csv('./data/keti_words_f.txt', sep='\\t', header=None)\n",
    "#keti_lists = pd.DataFrame(keti_list.loc[:,4])\n",
    "keti1 = pd.DataFrame(keti.iloc[:,6])\n",
    "keti_list.columns= ['한국어']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(keti,keti_list, on='한국어', how='inner')\n",
    "#result['언어 제공자 ID'].apply(lambda x : re.sub(\"[.]\", \"31\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.sort_values(by=['언어 제공자 ID', '번호']).to_csv('data/result_words2017.csv', index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "words = pd.read_csv('data/result_words.csv', header = None)\n",
    "words.iloc[0:26, 6].to_csv('data/keti_words_list.csv', index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "client_key = 'izGsqP2exeThwwEUVU3x'\n",
    "client_secret = 'WrwbQ1l6ZI'\n",
    "query = '진료 수화'\n",
    "encText = urllib.parse.quote_plus(query) # quote를 써도됨 공백이 없기 때문에  \n",
    "num = 100\n",
    "naver_url = 'https://openapi.naver.com/v1/search/blog.json?query=' + encText + '&display=' + str(num)\n",
    "\n",
    "text = []\n",
    "request = urllib.request.Request(naver_url)\n",
    "request.add_header(\"X-Naver-Client-Id\",client_key)\n",
    "request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "response = urllib.request.urlopen(request)\n",
    "rescode = response.getcode()\n",
    "if(rescode == 200):\n",
    "    response_body = response.read()\n",
    "    dataList = json.loads(response_body)\n",
    "    count = 1\n",
    "    print('[' + query + '에 대한 네이버 블로그 글 ]')\n",
    "        \n",
    "    for data in dataList['items'] :\n",
    "        #print (str(count) + ' : ' + data['title'], end=\" \")\n",
    "        text.append(data['title'])\n",
    "        #print ('[' + data['description'] + ']')\n",
    "        text.append(data['description'])\n",
    "        count += 1\n",
    "else:\n",
    "    print('오류 코드 : ' + rescode)\n",
    "    \n",
    "wfile = open(\"output/collected.txt\",\"at\", encoding=\"utf-8\") \n",
    "wfile.writelines(text) \n",
    "wfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "client_key = 'izGsqP2exeThwwEUVU3x'\n",
    "client_secret = 'WrwbQ1l6ZI'\n",
    "query = '진료 수화'\n",
    "encText = urllib.parse.quote_plus(query)\n",
    "\n",
    "num = 100\n",
    "naver_url = 'https://openapi.naver.com/v1/search/news.json?query=' + encText + '&display=' + str(num)\n",
    "\n",
    "text = []\n",
    "request = urllib.request.Request(naver_url)\n",
    "request.add_header(\"X-Naver-Client-Id\",client_key)\n",
    "request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "response = urllib.request.urlopen(request)\n",
    "\n",
    "rescode = response.getcode()\n",
    "\n",
    "if(rescode == 200):\n",
    "    response_body = response.read()\n",
    "    dataList = json.loads(response_body)\n",
    "    count = 1\n",
    "    print('[' + query + '에 대한 네이버 뉴스 글 ]')\n",
    "    for data in dataList['items'] :\n",
    "        #print (str(count) + ' : ' + data['title'], end=\" \")\n",
    "        text.append(data['title'])\n",
    "        #print ('[' + data['description'] + ']')\n",
    "        text.append(data['description'])\n",
    "        count += 1\n",
    "else:\n",
    "    print('오류 코드 : ' + rescode)\n",
    "\n",
    "wfile = open(\"output/collected.txt\",\"at\", encoding=\"utf-8\") \n",
    "wfile.writelines(text) \n",
    "wfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys \n",
    "import time\n",
    "import re\n",
    "\n",
    "url = \"https://search.naver.com/search.naver?where=kin&sm=tab_jum&query=진료 수화\"\n",
    "\n",
    "driver = webdriver.Chrome('C:/Temp/chromedriver')\n",
    "driver.implicitly_wait(5) \n",
    "\n",
    "driver.get(url) \n",
    "\n",
    "works = []\n",
    "while True:\n",
    "    titles = driver.find_elements_by_css_selector(\"#main_pack > section > div > ul > li > div > div.question_area > div > a\")\n",
    "\n",
    "    reviews = driver.find_elements_by_css_selector(\"#main_pack > section > div > ul > li > div > div.answer_area > div.answer_group > a\")\n",
    "\n",
    "    for title in titles:\n",
    "        #print(title.text,sep='')\n",
    "        works.append(title.text)\n",
    "    for review in reviews:\n",
    "        #print(review.text,sep='', end=' ')\n",
    "        works.append(title.text)\n",
    "        \n",
    "    nextbtn = driver.find_element_by_css_selector(\"#main_pack > div.api_sc_page_wrap > div > a.btn_next\")\n",
    "    if nextbtn.get_attribute('aria-disabled') == \"true\":\n",
    "        print(\"work finished\")\n",
    "        break\n",
    "    else:\n",
    "        driver.execute_script(\"arguments[0].click();\", nextbtn)\n",
    "        #time.sleep(1)\n",
    "    \n",
    "wfile = open(\"output/collected.txt\",\"at\", encoding=\"utf-8\") \n",
    "wfile.writelines(works) \n",
    "wfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt    \n",
    "from konlpy.tag import Hannanum      \n",
    "from konlpy.tag import Kkma      \n",
    "from konlpy.utils import pprint\n",
    "from wordcloud import WordCloud  \n",
    "import nltk      \n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('output/collected.txt', 'r', encoding='utf-8')\n",
    "text = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(\"[^가-힣\\s]\", \"\", text)\n",
    "wfile = open(\"output/collect_edit.txt\",\"w\", encoding=\"utf-8\") \n",
    "wfile.writelines(text) \n",
    "wfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfontpath = \"data/THEdog.ttf\" \n",
    "okt = Okt()\n",
    "#hannanum = Hannanum()\n",
    "#kkma = Kkma()\n",
    "tokens_ko = okt.nouns(text)                ## 텍스트에서 명사를 추출한다. \n",
    "#tokens_ko = hannanum.nouns(text)\n",
    "#tokens_ko = kkma.nouns(text)\n",
    "print(tokens_ko[:30])\n",
    "\n",
    "nouns_text = nltk.Text(tokens_ko, name='naver hotel reviews')       ## 명사로 추출한 것을 텍스트 객체로 만든다 \n",
    "#nouns_text.vocab()\n",
    "nouns_text.vocab().most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokens_ko)-1, 1, -1):\n",
    "    if len(tokens_ko[i]) < 2:\n",
    "        #tokens_ko.remove(tokens_ko[i])\n",
    "        del tokens_ko[i]\n",
    "        \n",
    "nouns_text = nltk.Text(tokens_ko)       ## 명사로 추출한 것을 텍스트 객체로 만든다 \n",
    "#nouns_text.vocab()\n",
    "nouns_text.vocab().most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = nouns_text.vocab().most_common(150) # 사용할 경우 dict로 형변환해서 넣어줘야함\n",
    "wc = WordCloud(font_path=myfontpath,\n",
    "                      relative_scaling = 0.2,\n",
    "                      width = 700,\n",
    "                      height = 500,\n",
    "                      colormap = 'coolwarm' # Set1, \n",
    "                      ).generate_from_frequencies(nouns_text.vocab())\n",
    "wc.to_file('output/hw8.png')\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image                                ## 이미지 파일을 처리하는 모듈을 사용한다. \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impo = np.array(Image.open('data/imposter.jpeg'))       ## 이미지를 읽어와서 다차원 배열로 변환한다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import STOPWORDS  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = nouns_text.vocab().most_common(150) # 사용할 경우 dict로 형변환해서 넣어줘야함\n",
    "wc = WordCloud(font_path=myfontpath,\n",
    "                      relative_scaling = 0.2,\n",
    "                      width = 700,\n",
    "                      height = 500,\n",
    "                      colormap = 'coolwarm', # Set1, \n",
    "                      #mask=impo\n",
    "                      ).generate_from_frequencies(nouns_text.vocab())\n",
    "#wc.to_file('output/hw8.png')\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
